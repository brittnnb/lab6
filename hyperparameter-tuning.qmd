---
title: "hyperparameter-tuning"
author: 'Brittany Bobb'
date: '2025-04-17'
format: 
    html:
      self-contained: true
execute:
  echo: true
editor: source
---

```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(dplyr)
library(readr)
library(purrr)
library(ggplot2)
library(patchwork)
library(ranger)
library(visdat)
library(ggpubr)
library(tune)
library(keras)
library(dials)
```
```{r}
#
#
#
###LAB 8 IS NEAR THE BOTTOM
#
#
#
```

```{r}
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')

```

```{r}
root  <- 'https://gdex.ucar.edu/dataset/camels/file'
```

```{r}
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
```

```{r}
# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')
```

```{r}
purrr::walk2(remote_files, local_files, download.file, quiet = TRUE)
```

```{r}
# Read and merge data
camels <-purrr::map(local_files, read_delim, show_col_types = FALSE) 
```

```{r}
camels <-powerjoin::power_full_join(camels ,by = 'gauge_id')
```

```{r}
# QUESTION 1 ANSWER:According to the CAMELS dataset documentation, zero_q_freq represents the fraction of time during which streamflow is zero for a given basin or site. This is important for understanding the behavior of the streamflow over time and can be used to identify basins that experience dry conditions or intermittent flow.

```

```{r}
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()
```




```{r}
# QUESTION 2
```

```{r}
# Create map for aridity
map_aridity <- ggplot(camels, aes(x = gauge_lon, y = gauge_lat, color = aridity)) +
  geom_point() +
  scale_color_viridis_c() + # Viridis scale for continuous data
  labs(title = "Aridity by Site", color = "Aridity") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  theme(axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank())

```

```{r}
# Create map for p_mean
map_pmean <- ggplot(camels, aes(x = gauge_lon, y = gauge_lat, color = p_mean)) +
  geom_point() +
  scale_color_viridis_c() + # Viridis scale for continuous data
  labs(title = "Mean Precipitation by Site", color = "Mean Precipitation") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  theme(axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank())

```

```{r}
# Combine the two maps into one figure
combined_map <- map_aridity + map_pmean + plot_layout(ncol = 2)

# Display the combined maps
print(combined_map)

```

```{r}
# Reshape data to long format
camels_long <- camels %>%
  gather(key = "parameter", value = "value", aridity, p_mean)

# Create facet map
map_facet <- ggplot(camels_long, aes(x = gauge_lon, y = gauge_lat, color = value)) +
  geom_point() +
  scale_color_viridis_c() +
  labs(color = "Value") +
  facet_wrap(~parameter, scales = "free") + 
  theme_minimal() +
  theme(legend.position = "bottom")

# Display the facet map
print(map_facet)

```


```{r}
# Model Preparation 
camels |> 
  select(aridity, p_mean, q_mean) |> 
  drop_na() |> 
  cor()
```

```{r}
# Visual EDA
# Create a scatter plot of aridity vs rainfall
ggplot(camels, aes(x = aridity, y = p_mean)) +
  # Add points colored by mean flow
  geom_point(aes(color = q_mean)) +
  # Add a linear regression line
  geom_smooth(method = "lm", color = "red", linetype = 2) +
  # Apply the viridis color scale
  scale_color_viridis_c() +
  # Add a title, axis labels, and theme (w/ legend on the bottom)
  theme_linedraw() + 
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

```{r}
# Test a transformation 
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c() +
  # Apply log transformations to the x and y axes
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  # Apply a log transformation to the color scale
  scale_color_viridis_c(trans = "log") +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom",
        # Expand the legend width ...
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(.5, "cm")) + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow") 
```

```{r}
# Model Building 
# splitting the data 
set.seed(123)
# Bad form to perform simple transformations on the outcome variable within a 
# recipe. So, we'll do it here.
camels <- camels |> 
  mutate(logQmean = log(q_mean))

# Generate the split
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)
```

```{r}
# Create a recipe to preprocess the data
#logQmean ~ aridity + p_mean:
#This formula indicates that the target variable (logQmean) is being predicted using the predictor variables aridity and p_mean.logQmean: The dependent or outcome variable (the one you're trying to predict).aridity and p_mean: The independent or predictor variables (the ones you use to make predictions).

rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())
```

```{r}
# Naive base lm approach 
# Prepare the data
baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)

# Interaction with lm
#  Base lm sets interaction terms with the * symbol
lm_base <- lm(logQmean ~ aridity * p_mean, data = baked_data)
summary(lm_base)
```

```{r}
# Sanity Interaction term from recipe ... these should be equal!!
summary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))
```

```{r}
# Where things get a little messy:
#The broom package provides a convenient way to extract model predictions and residuals. We can use the augment function to add predicted values to the test data. However, if we use augment directly on the test data, we will get incorrect results because the preprocessing steps defined in the recipe object have not been applied to the test data.
#Error in `$<-`:! Assigned data `predict(x, na.action = na.pass, ...) %>% unname()` must  be compatible with existing data.✖ Existing data has 135 rows.✖ Assigned data has 535 rows.ℹ Only vectors of size 1 are recycled.Caused by error in `vectbl_recycle_rhs_rows()`:! Can't recycle input of size 535 to size 135.

#The predict function can be used to make predictions on new data. However, if we use predict directly on the test data, we will get incorrect results because the preprocessing steps defined in the recipe object have not been applied to the test data.
```

```{r}
# Correct version: prep, bake, predict 
#To correctly evaluate the model on the test data, we need to apply the same preprocessing steps to the test data that we applied to the training data. We can do this using the prep and bake functions with the recipe object. This ensures the test data is transformed in the same way as the training data before making predictions.
```

```{r}
test_data <-  bake(prep(rec), new_data = camels_test)
test_data$lm_pred <- predict(lm_base, newdata = test_data)
```

```{r}
# Model Evaluations : statistical vs visual
#Now that we have the predicted values, we can evaluate the model using the metrics function from the yardstick package. This function calculates common regression metrics such as RMSE, R-squared, and MAE between the observed and predicted values.
metrics(test_data, truth = logQmean, estimate = lm_pred)
```

```{r}
ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +
  # Apply a gradient color scale
  scale_color_gradient2(low = "brown", mid = "orange", high = "darkgreen") +
  geom_point() +
  geom_abline(linetype = 2) +
  theme_linedraw() + 
  labs(title = "Linear Model: Observed vs Predicted",
       x = "Observed Log Mean Flow",
       y = "Predicted Log Mean Flow",
       color = "Aridity")
```

```{r}
# Ok so that was a bit burdensome, is really error prone (fragile), and is worthless if we wanted to test a different algorithm… lets look at a better approach!
```

```{r}
# Using a workflow instead 
# Define model
lm_model <- linear_reg() %>%
  # define the engine
  set_engine("lm") %>%
  # define the mode
  set_mode("regression")

# Instantiate a workflow ...
lm_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(lm_model) %>%
  # Fit the model to the training data
  fit(data = camels_train) 

# Extract the model coefficients from the workflow
summary(extract_fit_engine(lm_wf))$coefficients
```

```{r}
# From the base implementation
summary(lm_base)$coefficients
```

```{r}
# Making Predictions 
#Now that lm_wf is a workflow, data is not embedded in the model, we can use augment with the new_data argument to make predictions on the test data.

lm_data <- augment(lm_wf, new_data = camels_test)
dim(lm_data)
```

```{r}
# Model Evaluations: statistical and visual 

#As with EDA, applying for graphical and statistical evaluation of the model is a key Here, we use the metrics function to extract the default metrics (rmse, rsq, mae) between the observed and predicted mean streamflow values.

#We then create a scatter plot of the observed vs predicted values, colored by aridity, to visualize the model performance.

metrics(lm_data, truth = logQmean, estimate = .pred)
ggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
# Switch it up: We define a random forest model using the rand_forest function, set the engine to ranger, and the mode to regression. We then add the recipe, fit the model, and evaluate the skill.

rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(rf_model) %>%
  # Fit the model
  fit(data = camels_train) 
```

```{r}
#Predictions 
#Make predictions on the test data using the augment function and the new_data argument.

rf_data <- augment(rf_wf, new_data = camels_test)
dim(rf_data)
```

```{r}
# Model Evaluation 
#Evaluate the model using the metrics function and create a scatter plot of the observed vs predicted values, colored by aridity.

metrics(rf_data, truth = logQmean, estimate = .pred)
```

```{r}
ggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
# A workflowset approach 
# workflow_set is a powerful tool for comparing multiple models on the same data. It allows you to define a set of workflows, fit them to the same data, and evaluate their performance using a common metric. Here, we are going to create a workflow_set object with the linear regression and random forest models, fit them to the training data, and compare their performance using the autoplot and rank_results functions.
```

```{r}
wf <- workflow_set(list(rec), list(lm_model, rf_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)
```

```{r}
rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```



```{r}
# QUESTION 3 
```

```{r}
library(xgboost)

# Boost model 

# Define model
b_model <- boost_tree() %>%
  # define the engine
  set_engine("xgboost") %>%
  # define the mode
  set_mode("regression")

# Instantiate a workflow ...
b_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(b_model) %>%
  # Fit the model to the training data
  fit(data = camels_train) 

b_data <- augment(b_wf, new_data = camels_test)
dim(b_data)

metrics(b_data, truth = logQmean, estimate = .pred)
```

```{r}
ggplot(b_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()

```

```{r}
# Neural Network 

# Define model
nn_model <- bag_mlp() %>%
  # define the engine
  set_engine("nnet") %>%
  # define the mode
  set_mode("regression")

# Instantiate a workflow ...
nn_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(nn_model) %>%
  # Fit the model to the training data
  fit(data = camels_train) 

nn_data <- augment(nn_wf, new_data = camels_test)
dim(nn_data)

metrics(nn_data, truth = logQmean, estimate = .pred)
```

```{r}
ggplot(nn_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
wf <- workflow_set(list(rec), list(lm_model, rf_model, b_model, nn_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)
```

```{r}
rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```



```{r}
# QUESTION 3 ANSWER:
# according to the ranked results the Neural Network seems to be preforming the best but 
# because there a black box element to NN I would probably choose the Random Forest Model 
# instead. The data set doesn't seem large enough to need to use a NN.
```

```{r}
# Build your own:
#Borrowing from the workflow presented above, build your own complete ML pipeline to predict mean streamflow using the CAMELS dataset. You can experiment with different predictors and preprocessing steps to see how they impact model performance. A successful model will have a R-squared value > 0.9. To get started, you can use the following steps as a template:
```

```{r}
# Data Splitting 

set.seed(1234)
camels2 <- camels |> 
  mutate(logQmean = log(q_mean))

# Generate the split
camels_split2 <- initial_split(camels2, prop = 0.75)
camels_train2 <- training(camels_split2)
camels_test2  <- testing(camels_split2)

camels_cv2 <- vfold_cv(camels_train2, v = 10)
```

```{r}
colnames(camels2)
```


```{r}
# Recipe:

# Define a formula to predict logQmean
rec2 <- recipe(logQmean ~ aridity + p_mean + pet_mean + elev_mean, data = camels_train2) %>%
#ANSWER: I chose to still use aridity and precipitation mean but added potential evapotranspiration mean and elevation mean to help predict stream flow. PET can help estimate how much water may be lost to the atmosphere which effects stream flow. I added elevation out of curiosity.
step_log(all_predictors()) %>%
  # Add an interaction term between aridity, p_mean, pet_mean, elev_mean
  step_interact(terms = ~ aridity:p_mean:pet_mean:elev_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())  
```

```{r}
# Define 3 models 

# Random Forest Model 
rf_model2 <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf2 <- workflow() %>%
  # Add the recipe
  add_recipe(rec2) %>%
  # Add the model
  add_model(rf_model2) %>%
  # Fit the model
  fit(data = camels_train2) 
```

```{r}
rf_data2 <- augment(rf_wf2, new_data = camels_test2)
dim(rf_data2)
```
```{r}
metrics(rf_data2, truth = logQmean, estimate = .pred)
```

```{r}
ggplot(rf_data2, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
#Boost model 

# Define model
b_model2 <- boost_tree() %>%
  # define the engine
  set_engine("xgboost") %>%
  # define the mode
  set_mode("regression")

# Instantiate a workflow ...
b_wf2 <- workflow() %>%
  # Add the recipe
  add_recipe(rec2) %>%
  # Add the model
  add_model(b_model2) %>%
  # Fit the model to the training data
  fit(data = camels_train2) 

b_data2 <- augment(b_wf2, new_data = camels_test2)
dim(b_data2)

metrics(b_data2, truth = logQmean, estimate = .pred)
```

```{r}
ggplot(b_data2, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
# Neural Network 

# Define model
nn_model2 <- bag_mlp() %>%
  # define the engine
  set_engine("nnet") %>%
  # define the mode
  set_mode("regression")

# Instantiate a workflow ...
nn_wf2 <- workflow() %>%
  # Add the recipe
  add_recipe(rec2) %>%
  # Add the model
  add_model(nn_model2) %>%
  # Fit the model to the training data
  fit(data = camels_train2) 

nn_data2 <- augment(nn_wf2, new_data = camels_test2)
dim(nn_data2)

metrics(nn_data2, truth = logQmean, estimate = .pred)

```

```{r}
ggplot(nn_data2, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
# Evaluate 

wf2 <- workflow_set(list(rec), list(rf_model2, b_model2, nn_model2)) %>%
  workflow_map('fit_resamples', resamples = camels_cv2) 

autoplot(wf2)
```

```{r}
rank_results(wf2, rank_metric = "rsq", select_best = TRUE)
```



```{r}
#ANSWER: I chose to work with the Neural Network model because it is prefomring the best and has the highest r squared value 
```


```{r}
# Extract and Evaluate Best Model

# Build the final workflow with the best model (e.g., NN)
best_wf <- workflow() %>%
  add_recipe(rec2) %>%
  add_model(nn_model2)

# Fit the final model to the full training data
final_fit <- fit(best_wf, data = camels_train2)

# Predict on the test set
best_data <- augment(final_fit, new_data = camels_test2)

# Plot observed vs predicted
ggplot(best_data, aes(x = logQmean, y = .pred)) +
  geom_point(color = "blue") +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(
    title = "Observed vs Predicted Log Streamflow (logQmean)",
    x = "Observed LogQmean",
    y = "Predicted LogQmean"
  ) +
  theme_minimal()

```



```{r}
#Answer: I'm pretty satisfied with the results there are a few outliers I would need to look into but overall I feel because most of the points are close to the LOB the models predictions are close to the actual values 
```

```{r}
#
# LAB 8
#Model Tuning 
#
#
```


```{r}
#
# Define a workflow object with the random forest model and recipe
#
```


```{r}
library(tidymodels)
library(ranger)


# Define the random forest model with tunable hyperparameters
rf_spec <- rand_forest(
  mtry = tune(),       # Number of predictors to consider at each split
  trees = tune(),      # Number of trees in the forest
  min_n = tune()       # Minimum number of observations per terminal node
) %>%
  set_engine("ranger") %>%
  set_mode("regression")
#Define a recipe for preprocessing 
# Define the recipe with proper transformation (log transformation on 'q_mean')
rec3 <- recipe(q_mean ~ aridity + p_mean + pet_mean + elev_mean, data = camels_train2) %>%
  step_log(q_mean, base = exp(1))  # Apply log transformation to q_mean

# Define the workflow combining the recipe and random forest model
rf_workflow <- workflow() %>%
  add_recipe(rec3) %>%
  add_model(rf_spec)

# Define the grid for hyperparameter search
rf_grid <- grid_regular(
  mtry(c(1, 4)),      # Number of predictors to consider (1 to 4)
  trees(c(50, 500)),  # Number of trees (50 to 500)
  min_n(c(2, 40)),    # Minimum number of observations per terminal node
  levels = 5           # Number of levels for each hyperparameter
)

# Create cross-validation folds
folds <- vfold_cv(camels_train2, v = 5)

# Perform hyperparameter tuning using the grid search
model_params <- tune_grid(
  rf_workflow,
  resamples = folds,
  grid = rf_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = TRUE)
)

# View the results of tuning
autoplot(model_params)
```

```{r}
#
#Check the Skill of the Tuned Model
#
# Check the metrics of the tuned model
model_metrics <- collect_metrics(model_params)
print(model_metrics)

# Show the best performing model based on Mean Absolute Error (MAE)
best_model <- show_best(model_params, metric = "mae")
print(best_model)

# Alternatively, use select_best() to get the best hyperparameter set
hp_best <- select_best(model_params, metric = "mae")
print(hp_best)


```


```{r}
#
#Finalize the Model
# 
# Finalize the workflow with the best hyperparameters
final_rf_workflow <- rf_workflow %>%
  finalize_workflow(hp_best)

# Fit the finalized model on the full training data
final_rf_fit <- final_rf_workflow %>%
  fit(camels_train2)

```

```{r}
#
#Model Verification on Test Data Using last_fit()
#

# Fit and validate the model using last_fit()
final_rf_last_fit <- last_fit(final_rf_workflow, camels_split2)

# Check the performance of the final model on the test data
final_metrics <- collect_metrics(final_rf_last_fit)
print(final_metrics)

```

```{r}
#
#Scatter Plot of Predicted vs Actual Values
#
# Get the predictions from the final model
final_predictions <- collect_predictions(final_rf_last_fit)

# Plot the predicted vs actual values
ggplot(final_predictions, aes(x = .pred, y = q_mean)) +
  geom_point(aes(color = .pred), alpha = 0.7) +
  geom_smooth(method = "lm", color = "blue") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  scale_color_viridis_c() +
  labs(x = "Predicted", y = "Actual", title = "Predicted vs Actual Values") +
  theme_minimal()

```




